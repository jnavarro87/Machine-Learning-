{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gensim 3.8.3 version para word2vec \n",
    "from pathlib import Path\n",
    "import chardet\n",
    "import es_core_news_sm\n",
    "from scipy.spatial.distance import cosine\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import regex\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "from typing import Dict, Union\n",
    "import requests\n",
    "from wikitextparser import remove_markup\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from typing import List, Optional, Sequence\n",
    "from functools import reduce\n",
    "from gensim import models\n",
    "from gensim import corpora\n",
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "from enum import Enum\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate texts from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikipedia_corpus():\n",
    "    \"\"\"Extrae contenido de textos de wikipedia.\n",
    "    \n",
    "    Los textos se escogen al azar, de wikipedia en español.\n",
    "    Para más información revisar la API:\n",
    "    \n",
    "    https://www.mediawiki.org/wiki/API:Random\n",
    "    \n",
    "    Se utiliza el paquete ``wikitextparser`` para limpiar\n",
    "    los textos (en formato wikitext):\n",
    "    \n",
    "    https://github.com/5j9/wikitextparser#miscellaneous\n",
    "    \"\"\"\n",
    "    url = \"https://es.wikipedia.org/w/api.php\"\n",
    "\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"generator\": \"random\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"grnnamespace\": \"0\",\n",
    "    }\n",
    "    session = requests.Session()\n",
    "    res = session.get(url=url, params=params)\n",
    "    res.raise_for_status()\n",
    "    data = res.json()\n",
    "    for id, page in data[\"query\"][\"pages\"].items():\n",
    "        if \"revisions\" in page:\n",
    "            yield remove_markup(page[\"revisions\"][0][\"*\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of list of texts from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_textos =1000\n",
    "textos_wiki = []\n",
    "id_o = 0\n",
    "for i in range(0,cant_textos):\n",
    "    texto = next(wikipedia_corpus())\n",
    "    textos_wiki.append(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Instanciate Class of lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEMMATIZATION_LIST_FILE = Path(\"lemmatization-es.txt\")\n",
    "\n",
    "class Lemmatizer(ABC):\n",
    "    \"\"\"Abstrae lematizador.\"\"\"\n",
    "    @abstractmethod\n",
    "    def lemmatize(self, word: str) -> str:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class DummyLemmatizer(Lemmatizer):\n",
    "    \"\"\"Lematizador que no hace nada.\"\"\"\n",
    "    def lemmatize(self, word: str) -> str:\n",
    "        return word\n",
    "\n",
    "\n",
    "class LexiconLemmatizer(Lemmatizer):\n",
    "    \"\"\"Implementa un lematizador basado en lexicon.\"\"\"\n",
    "\n",
    "    def __init__(self, lemma_dict: Dict[str, str]):\n",
    "        \"\"\"Constructor.\n",
    "        \n",
    "        Lee un diccionario de reglas y lo utiliza como lematizador.\n",
    "        \n",
    "        :param lemma_dict: Diccionario de reglas de lematización.\n",
    "        :type lemma_dict: Dict[str, str] \n",
    "        \"\"\"\n",
    "        self.lemma_dict = lemma_dict\n",
    "        \n",
    "    @classmethod\n",
    "    def from_file(cls: \"LexiconLemmatizer\",\n",
    "                  filepath: Union[str, Path]) -> \"LexiconLemmatizer\":\n",
    "        \"\"\"Carga un lematizador desde un archivo de texto.\"\"\"\n",
    "        if isinstance(filepath, str):\n",
    "            filepath = Path(filepath)\n",
    "            \n",
    "        with open(filepath, \"r\") as fp:\n",
    "            lemma_dict = dict()\n",
    "            for line in fp:\n",
    "                try:\n",
    "                    lemma, word = line.strip().split()\n",
    "                    lemma_dict[word] = lemma\n",
    "                except Exception:\n",
    "                    pass\n",
    "        return cls(lemma_dict)\n",
    "\n",
    "    def lemmatize(self, word: str) -> str:\n",
    "        \"\"\"Lematiza una palabra.\n",
    "\n",
    "        Si la palabra no se encuentra en el lexicon utilizado para\n",
    "        inicializar el lematizador, retornará la misma palabra como\n",
    "        lemma.\n",
    "\n",
    "        :param word: Palabra a lematizar\n",
    "        :type word: str\n",
    "        :return: Lema de la palabra\n",
    "        :rtype: str\n",
    "        \"\"\"\n",
    "        return self.lemma_dict.get(word, word)\n",
    "\n",
    "\n",
    "nlp           = es_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'manipular'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying Lemmatizer\n",
    "lemmatizer = LexiconLemmatizer.from_file(LEMMATIZATION_LIST_FILE)\n",
    "lemmatizer.lemmatize(\"manipulaste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['perro', 'comer', 'carne']"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pre_processing_pipeline(*preprocessing_steps):\n",
    "    \"\"\"Implementa composicion de funciones.\n",
    "    \n",
    "    En escencia recibe un iterable de funciones, y las aplica\n",
    "    en el orden entregado, por ejemplo:\n",
    "    \n",
    "    - Limpiar texto\n",
    "    - Tokenizar\n",
    "    \n",
    "    Tomará el texto de entrada lo limpiará y lo tokenizará.\n",
    "    \"\"\"\n",
    "    return reduce(\n",
    "        lambda f, g: lambda x: g(f(x)),\n",
    "        preprocessing_steps, lambda x: x)\n",
    "\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"Reemplaza signos de puntuación por espacios.\"\"\"\n",
    "    return text.translate(\n",
    "        str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens: List[str]) -> Sequence[str]:\n",
    "    \"\"\"Filtra palabras que sean stopwords.\"\"\"\n",
    "    return filter(lambda x: x not in stopwords.words(\"spanish\"), tokens)\n",
    "    \n",
    "\n",
    "def lemmatize(tokens: Sequence[str],\n",
    "              lemmatizer: Optional[Lemmatizer]=None) -> Sequence[str]:\n",
    "    \"\"\"Lematiza tokens.\"\"\"\n",
    "    if lemmatizer is None:\n",
    "        lemmatizer = DummyLemmatizer()\n",
    "        \n",
    "    return map(lemmatizer.lemmatize, tokens)\n",
    "\n",
    "def lowercase(text: str) -> str:\n",
    "    \"\"\"Transforma texto a minúsculas.\"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Tokeniza texto.\"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\n",
    "# Pipeline de preprocesamiento\n",
    "pipeline = pre_processing_pipeline(\n",
    "    remove_punctuation,\n",
    "    lowercase,\n",
    "    tokenize,\n",
    "    remove_stopwords,\n",
    "    lambda x: lemmatize(x, lemmatizer))\n",
    "\n",
    "\n",
    "# Ejemplo\n",
    "list(pipeline(\"El perro come carne\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count token frequency and remove those that only appear once, they can be considered \"noise\".  Then train word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18672151, 19393450)"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency = defaultdict(int)\n",
    "for text in map(pipeline, textos_wiki):\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [\n",
    "    [token for token in pipeline(document) if frequency[token] > 1]\n",
    "    for document in textos_wiki\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training a word2vec model from the given data set\n",
    "w2v_model = Word2Vec(texts, size=200, min_count=2, window=4, sg=1, workers=4)\n",
    "w2v_model.train(texts, total_examples=len(texts), compute_loss=True, epochs=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = [term for term in w2v_model.wv.vocab]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create class for recognize type of question and create function for embedding of the phrases, tokens or text with Word2vec model, to get embedding of the text(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionType(Enum):\n",
    "    QUE = \"QUÉ\"\n",
    "    QUIEN = \"QUIÉN\"\n",
    "    DONDE = \"DÓNDE\"\n",
    "    COMO = \"CÓMO\"    \n",
    "    CUANDO = \"CUANDO\"   \n",
    "    PORQUE = \"PORQUÉ\"    \n",
    "    CUAL = \"CUÁL\"\n",
    "\n",
    "\n",
    "class QueryClassification(ABC):\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def get_query_class(query: str) -> QuestionType:\n",
    "        pass\n",
    "\n",
    "    \n",
    "class RuleQueryClassification(QueryClassification):\n",
    "    rules = {\n",
    "        r\"^¿Qué (\\w\\s?)+\\?$\": QuestionType.QUE,\n",
    "        r\"^¿Quién (\\w\\s?)+\\?$\": QuestionType.QUIEN,\n",
    "        r\"^¿Dónde (\\w\\s?)+\\?$\": QuestionType.DONDE,\n",
    "        r\"^¿Cómo (\\w\\s?)+\\?$\": QuestionType.COMO,        \n",
    "        r\"^¿Cuando (\\w\\s?)+\\?$\": QuestionType.CUANDO,        \n",
    "        r\"^¿Porqué (\\w\\s?)+\\?$\": QuestionType.PORQUE,        \n",
    "        r\"^¿Cual (\\w\\s?)+\\?$\": QuestionType.CUAL,\n",
    "    }\n",
    "    \n",
    "    def get_query_class(query: str) -> QuestionType:\n",
    "        for rule, query_type in RuleQueryClassification.rules.items():\n",
    "            if re.match(rule, query, re.IGNORECASE):\n",
    "                return query_type\n",
    "        raise ValueError(f\"Tipo de pregunta no soportada: {query}\") \n",
    "        \n",
    "def ObtenerEmbeddingOracion(modelo, oracion):  \n",
    "    Lista_vectores = [modelo.wv[w]  for w in pipeline(oracion) if w in dictionary]\n",
    "    embedding_palabras = np.array(Lista_vectores)\n",
    "    embedding_oracion = embedding_palabras.mean(axis=0)\n",
    "    return(embedding_oracion) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<QuestionType.QUIEN: 'QUIÉN'>"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"¿Quién es Jose Velazquez?\"\n",
    "vec_query = ObtenerEmbeddingOracion(w2v_model,query)\n",
    "\n",
    "RuleQueryClassification.get_query_class(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure the cosine similarity between the question made against each vector of the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:39<00:00,  4.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "i=0\n",
    "Similitud=[]\n",
    "nombre=[]\n",
    "relevant_docs=[]\n",
    "threshold = 0.70\n",
    "relevant_index=[]\n",
    "relevant_sim=[]\n",
    "\n",
    "for x in tqdm(textos_wiki):\n",
    "    texto=ObtenerEmbeddingOracion(w2v_model, x)\n",
    "    similitud = cosine_similarity([texto], [vec_query])\n",
    "    Similitud.append(similitud)\n",
    "    if similitud > threshold:\n",
    "        relevant_index.append(i)\n",
    "        relevant_sim.append(similitud)\n",
    "        relevant_docs.append(x)\n",
    "    i+=1  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevant_docs</th>\n",
       "      <th>relevant_sim</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relevant_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>\\n\\nPacto de fuga es una película chilena del ...</td>\n",
       "      <td>[[0.7163151]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    relevant_docs  \\\n",
       "relevant_index                                                      \n",
       "86              \\n\\nPacto de fuga es una película chilena del ...   \n",
       "\n",
       "                 relevant_sim  \n",
       "relevant_index                 \n",
       "86              [[0.7163151]]  "
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similitud_indice = pd.DataFrame()\n",
    "similitud_indice[\"relevant_index\"] = relevant_index\n",
    "similitud_indice[\"relevant_docs\"] = relevant_docs\n",
    "similitud_indice[\"relevant_sim\"] = relevant_sim\n",
    "similitud_indice.set_index(\"relevant_index\").sort_values(by=[\"relevant_sim\"], ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With the selected text, measure the cosine similarity, between the query and each sentence from the selected text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.73it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(relevant_index):\n",
    "    sentences = sent_tokenize(textos_wiki[i])\n",
    "    relevant_passages = []\n",
    "    threshold_passage = 0.65\n",
    "    relevant_sim_pas=[]\n",
    "    pasajes_seleccionados = pd.DataFrame()\n",
    "    for sentence in sentences:\n",
    "        vec_passage = ObtenerEmbeddingOracion(w2v_model,sentence)\n",
    "        if np.all(np.isnan(vec_passage) != True): #este hay que arreglarlo \n",
    "            if cosine_similarity([vec_passage], [vec_query]) > threshold_passage:\n",
    "                relevant_passages.append(sentence)\n",
    "                relevant_sim_pas.append(cosine_similarity([vec_passage], [vec_query]))\n",
    "\n",
    "pasajes_seleccionados[\"relevant_passages\"] = relevant_passages\n",
    "pasajes_seleccionados[\"relevant_sim_pas\"] = relevant_sim_pas\n",
    "pasajes_seleccionados = pasajes_seleccionados.sort_values(by=[\"relevant_sim_pas\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m¿Quién es Jose Velazquez?\n",
      "\n",
      "\n",
      "\u001b[0m* Jose Luis Aguilera como gendarme Care'Poker.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevant_passages</th>\n",
       "      <th>relevant_sim_pas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>* Jose Luis Aguilera como gendarme Care'Poker.</td>\n",
       "      <td>[[0.7956158]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Simultáneamente  el fiscal Ad Hoc Andrade (Mat...</td>\n",
       "      <td>[[0.70753837]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   relevant_passages relevant_sim_pas\n",
       "1     * Jose Luis Aguilera como gendarme Care'Poker.    [[0.7956158]]\n",
       "0  Simultáneamente  el fiscal Ad Hoc Andrade (Mat...   [[0.70753837]]"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\033[1m' + query)\n",
    "print(\"\\n\")\n",
    "print('\\033[0m' + pasajes_seleccionados.iloc[0][\"relevant_passages\"])\n",
    "pasajes_seleccionados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
